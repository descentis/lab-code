{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing the required libraries"
      ],
      "metadata": {
        "id": "ZbyTSnFrwrRf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivoSQIm6wo9g",
        "outputId": "b4d1ee39-2d8a-4637-dac8-a1b33cab14e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[sentencepiece]) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[sentencepiece]) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the pipeline"
      ],
      "metadata": {
        "id": "xhKHnsBTxMPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "hi-Ofn0z1o7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-1"
      ],
      "metadata": {
        "id": "A727Pij-1ukk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the following code\n",
        "# Write about the model that you will be using\n",
        "\n",
        "# function definition\n",
        "def get_sentiment(sentences:list):\n",
        "\n",
        "  # Loading the pre-trained sentiment-analysis pipeline\n",
        "  sentiment_model = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "  # Get sentiment scores for each sentence\n",
        "  sentiment_scores = sentiment_model(sentences)\n",
        "\n",
        "  return sentiment_scores"
      ],
      "metadata": {
        "id": "35hgMCdKxdiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['My favorite bar in town love the live music and the martinis - fave is the strawberry shortcake!',\n",
        "             'BED BUGS!!! Horrible place! DO NOT STAY HERE!! Stayed here for a wedding we attended.',\n",
        "             'This was by far the worst hotel experience I\\'ve ever had.',\n",
        "             'AVOID THIS PLACE LIKE THEY SERVE SALMONELLA!',\n",
        "             'A great independent music store.  Really good selection',\n",
        "             'The best pawn shop in Las Vegas',\n",
        "             'This place is GREAT! Got rid of some jewelry I never wear and they got me the beat price for it.']\n",
        "\n",
        "# Function call\n",
        "scores = get_sentiment(sentences)\n",
        "\n",
        "#print(scores)\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  print(\"\")\n",
        "  print(\"Sentence : \" + sentences[i])\n",
        "  print(\"Sentiment : \" + scores[i]['label'] + \" ; \" + \"Score : \" + str(scores[i]['score']))"
      ],
      "metadata": {
        "id": "P2Z6XSugy9o0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d67cd5c3-8bc0-4bef-d697-eb62cef7d43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence : My favorite bar in town love the live music and the martinis - fave is the strawberry shortcake!\n",
            "Sentiment : POSITIVE ; Score : 0.9992315769195557\n",
            "\n",
            "Sentence : BED BUGS!!! Horrible place! DO NOT STAY HERE!! Stayed here for a wedding we attended.\n",
            "Sentiment : NEGATIVE ; Score : 0.9993444085121155\n",
            "\n",
            "Sentence : This was by far the worst hotel experience I've ever had.\n",
            "Sentiment : NEGATIVE ; Score : 0.9997853636741638\n",
            "\n",
            "Sentence : AVOID THIS PLACE LIKE THEY SERVE SALMONELLA!\n",
            "Sentiment : NEGATIVE ; Score : 0.9995777010917664\n",
            "\n",
            "Sentence : A great independent music store.  Really good selection\n",
            "Sentiment : POSITIVE ; Score : 0.9998502731323242\n",
            "\n",
            "Sentence : The best pawn shop in Las Vegas\n",
            "Sentiment : POSITIVE ; Score : 0.9998127818107605\n",
            "\n",
            "Sentence : This place is GREAT! Got rid of some jewelry I never wear and they got me the beat price for it.\n",
            "Sentiment : POSITIVE ; Score : 0.9998098015785217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-2"
      ],
      "metadata": {
        "id": "3l4zUZfN1xEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the following definition\n",
        "# Write about the model that you will be using\n",
        "def get_answers(context:str, question:str):\n",
        "\n",
        "  # Loading the question answering pipeline\n",
        "  classifier = pipeline('question-answering', model='google-bert/bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "  # Use the pipeline to get the answer\n",
        "  result = classifier(question=question, context=context)\n",
        "\n",
        "  # Extract the answer from the result\n",
        "  answer = result['answer']\n",
        "\n",
        "  return answer"
      ],
      "metadata": {
        "id": "y0qIIMu41-9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"In 2024 Hopfield, along with Geoffrey Hinton, was awarded the Nobel Prize in Physics for their foundational contributions to machine learning, particularly through their work on artificial neural networks.\"\n",
        "question = \"Who was awarded the nobel prize in physics?\"\n",
        "#question = \"Mention all Who was awarded the nobel prize in physics?\"\n",
        "#question = \"Why was awarded the Nobel prize?\"\n",
        "\n",
        "answer = get_answers(context, question)\n",
        "print(\"\")\n",
        "print(\"Question : \" + question)\n",
        "print(\"Answe : \" + answer )"
      ],
      "metadata": {
        "id": "dG3ofLM-2PpJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00be8b49-728e-41f0-fa4b-e4021af15e58"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question : Who was awarded the nobel prize in physics?\n",
            "Answe : Hopfield\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-3"
      ],
      "metadata": {
        "id": "OHWm4z8k3hi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "# Loading the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "\n",
        "# Loading the pre-trained model\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "\n",
        "def get_NER(sentences2:list):\n",
        "  # Creating pipeline for NER\n",
        "  classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "  # Get NER results\n",
        "  NER_scores = classifier(sentences2)\n",
        "  return NER_scores\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMN8MnKShAaL",
        "outputId": "f52a3488-ae84-4a42-a1bd-6ebdabf3fc8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences2 = [\"My name is John and I live in Texas\" , \"My name is Clara and I live in Berkeley, California.\"]\n",
        "nerResults = get_NER(sentences2)\n",
        "#print(nerResults)\n",
        "for results in nerResults:\n",
        "  for wordAndEntity in results:\n",
        "    #print(\"i\")\n",
        "    print(\"Word : \" + wordAndEntity['word'] + \" ; \" + \"Entity : \" + wordAndEntity['entity_group'] + \" ; \" + \"Score : \" + str(wordAndEntity['score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmY4m2knaoHP",
        "outputId": "c5731398-56a4-4b59-831b-18354c8298b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word : John ; Entity : PER ; Score : 0.9986386\n",
            "Word : Texas ; Entity : LOC ; Score : 0.9996892\n",
            "Word : Clara ; Entity : PER ; Score : 0.99641764\n",
            "Word : Berkeley ; Entity : LOC ; Score : 0.996198\n",
            "Word : California ; Entity : LOC ; Score : 0.9990196\n"
          ]
        }
      ]
    }
  ]
}